{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Waiting for a Spark session to start...\nSpark Initialization Done! ApplicationId = app-20200405172126-0000\nKERNEL_ID = 3465d6be-ca08-41d9-9ef1-91f5c270cae5\nCloning into 'HMP_dataset'...\nremote: Enumerating objects: 865, done.\u001b[K\nremote: Total 865 (delta 0), reused 0 (delta 0), pack-reused 865\u001b[K\nReceiving objects: 100% (865/865), 1010.96 KiB | 0 bytes/s, done.\nChecking out files: 100% (848/848), done.\n"
                }
            ],
            "source": "! git clone https://github.com/wchill/HMP_dataset.git"
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Brush_teeth\tDrink_glass  Liedown_bed  Sitdown_chair  final.py\r\nClimb_stairs\tEat_meat     MANUAL.txt   Standup_chair  impdata.py\r\nComb_hair\tEat_soup     Pour_water   Use_telephone\r\nDescend_stairs\tGetup_bed    README.txt   Walk\r\n"
                }
            ],
            "source": "! ls HMP_dataset"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "In this work, the pipeline for preprocessing of the accelerometer for various tasks is done"
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Accelerometer-2011-04-11-13-28-18-brush_teeth-f1.txt\r\nAccelerometer-2011-04-11-13-29-54-brush_teeth-f1.txt\r\nAccelerometer-2011-05-30-08-35-11-brush_teeth-f1.txt\r\nAccelerometer-2011-05-30-09-36-50-brush_teeth-f1.txt\r\nAccelerometer-2011-05-30-10-34-16-brush_teeth-m1.txt\r\nAccelerometer-2011-05-30-21-10-57-brush_teeth-f1.txt\r\nAccelerometer-2011-05-30-21-55-04-brush_teeth-m2.txt\r\nAccelerometer-2011-05-31-15-16-47-brush_teeth-f1.txt\r\nAccelerometer-2011-06-02-10-42-22-brush_teeth-f1.txt\r\nAccelerometer-2011-06-02-10-45-50-brush_teeth-f1.txt\r\nAccelerometer-2011-06-06-10-45-27-brush_teeth-f1.txt\r\nAccelerometer-2011-06-06-10-48-05-brush_teeth-f1.txt\r\n"
                }
            ],
            "source": "! ls HMP_dataset/Brush_teeth"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### So, now, iteratively progressing  through these folders to create the spark dataframes"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "1. Create Schema"
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": "from pyspark.sql.types import StructType, StructField, IntegerType"
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": "schema=StructType([\n        StructField('x', IntegerType(), True),\n        StructField('y', IntegerType(), True),\n        StructField('z', IntegerType(), True)\n])"
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": "import os"
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "['.git',\n '.idea',\n 'Brush_teeth',\n 'Climb_stairs',\n 'Comb_hair',\n 'Descend_stairs',\n 'Drink_glass',\n 'Eat_meat',\n 'Eat_soup',\n 'Getup_bed',\n 'Liedown_bed',\n 'MANUAL.txt',\n 'Pour_water',\n 'README.txt',\n 'Sitdown_chair',\n 'Standup_chair',\n 'Use_telephone',\n 'Walk',\n 'final.py',\n 'impdata.py']"
                    },
                    "execution_count": 13,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "file_list=os.listdir('HMP_dataset')\nfile_list"
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "['Brush_teeth',\n 'Climb_stairs',\n 'Comb_hair',\n 'Descend_stairs',\n 'Drink_glass',\n 'Eat_meat',\n 'Eat_soup',\n 'Getup_bed',\n 'Liedown_bed',\n 'Pour_water',\n 'Sitdown_chair',\n 'Standup_chair',\n 'Use_telephone']"
                    },
                    "execution_count": 19,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "#lets get rid of unwanted datas\nfile_list_filtered=[s for s in file_list if '_' in s]\nfile_list_filtered"
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [],
            "source": "#initiate empty df\ndf=None\n\n# now iterate over these file categories and inside categories, the datafiles\nfrom pyspark.sql.functions import lit\n\nfor category in file_list_filtered:\n    data_files=os.listdir('HMP_dataset/'+category)\n        #for each file we have a temperory data frame\n    for data_file in data_files:    \n        temp_df=spark.read.option('header', 'false').option('delimiter', \" \").csv('HMP_dataset/'+category+'/'+data_file, schema = schema)\n        #now add the source file (file name) and the y values i.e. categories too to the dataframes here using lit function.\n        temp_df= temp_df.withColumn('class', lit(category)) #added the dependent values here\n        temp_df= temp_df.withColumn('source', lit(data_file)) #added the source file\n\n        if df is None:\n            df=temp_df\n        else:\n            df=df.union(temp_df)"
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+---+---+---+-----------+--------------------+\n|  x|  y|  z|      class|              source|\n+---+---+---+-----------+--------------------+\n| 22| 49| 35|Brush_teeth|Accelerometer-201...|\n| 22| 49| 35|Brush_teeth|Accelerometer-201...|\n| 22| 52| 35|Brush_teeth|Accelerometer-201...|\n| 22| 52| 35|Brush_teeth|Accelerometer-201...|\n| 21| 52| 34|Brush_teeth|Accelerometer-201...|\n| 22| 51| 34|Brush_teeth|Accelerometer-201...|\n| 20| 50| 35|Brush_teeth|Accelerometer-201...|\n| 22| 52| 34|Brush_teeth|Accelerometer-201...|\n| 22| 50| 34|Brush_teeth|Accelerometer-201...|\n| 22| 51| 35|Brush_teeth|Accelerometer-201...|\n| 21| 51| 33|Brush_teeth|Accelerometer-201...|\n| 20| 50| 34|Brush_teeth|Accelerometer-201...|\n| 21| 49| 33|Brush_teeth|Accelerometer-201...|\n| 21| 49| 33|Brush_teeth|Accelerometer-201...|\n| 20| 51| 35|Brush_teeth|Accelerometer-201...|\n| 18| 49| 34|Brush_teeth|Accelerometer-201...|\n| 19| 48| 34|Brush_teeth|Accelerometer-201...|\n| 16| 53| 34|Brush_teeth|Accelerometer-201...|\n| 18| 52| 35|Brush_teeth|Accelerometer-201...|\n| 18| 51| 32|Brush_teeth|Accelerometer-201...|\n+---+---+---+-----------+--------------------+\nonly showing top 20 rows\n\n"
                }
            ],
            "source": "# LAZY spark\ndf.show()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### The next step is to transform the  data. \n\n2. Integer representation of string. I.e Label Encoding of the class values"
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+---+---+---+-----------+--------------------+----------+\n|  x|  y|  z|      class|              source|ClassIndex|\n+---+---+---+-----------+--------------------+----------+\n| 22| 49| 35|Brush_teeth|Accelerometer-201...|       5.0|\n| 22| 49| 35|Brush_teeth|Accelerometer-201...|       5.0|\n| 22| 52| 35|Brush_teeth|Accelerometer-201...|       5.0|\n| 22| 52| 35|Brush_teeth|Accelerometer-201...|       5.0|\n| 21| 52| 34|Brush_teeth|Accelerometer-201...|       5.0|\n| 22| 51| 34|Brush_teeth|Accelerometer-201...|       5.0|\n| 20| 50| 35|Brush_teeth|Accelerometer-201...|       5.0|\n| 22| 52| 34|Brush_teeth|Accelerometer-201...|       5.0|\n| 22| 50| 34|Brush_teeth|Accelerometer-201...|       5.0|\n| 22| 51| 35|Brush_teeth|Accelerometer-201...|       5.0|\n| 21| 51| 33|Brush_teeth|Accelerometer-201...|       5.0|\n| 20| 50| 34|Brush_teeth|Accelerometer-201...|       5.0|\n| 21| 49| 33|Brush_teeth|Accelerometer-201...|       5.0|\n| 21| 49| 33|Brush_teeth|Accelerometer-201...|       5.0|\n| 20| 51| 35|Brush_teeth|Accelerometer-201...|       5.0|\n| 18| 49| 34|Brush_teeth|Accelerometer-201...|       5.0|\n| 19| 48| 34|Brush_teeth|Accelerometer-201...|       5.0|\n| 16| 53| 34|Brush_teeth|Accelerometer-201...|       5.0|\n| 18| 52| 35|Brush_teeth|Accelerometer-201...|       5.0|\n| 18| 51| 32|Brush_teeth|Accelerometer-201...|       5.0|\n+---+---+---+-----------+--------------------+----------+\nonly showing top 20 rows\n\n"
                }
            ],
            "source": "from pyspark.ml.feature import StringIndexer #for label encoding\n\nindexer=StringIndexer(inputCol='class', outputCol='ClassIndex')\nindexed=indexer.fit(df).transform(df)\n\nindexed.show()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "3. One Hot encoding of the class Labels obtained in previous step. Here, the sparse matrix of the labels are done. \n\nThey are in form of example: [12, 3, 1]- meaning: 12 digits, on 3rd position, there is a 1\n\nPS: One hot encoder of pyspark has only transform function"
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+---+---+---+-----------+--------------------+----------+--------------+\n|  x|  y|  z|      class|              source|ClassIndex|   CategoryVec|\n+---+---+---+-----------+--------------------+----------+--------------+\n| 22| 49| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n| 22| 49| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n| 22| 52| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n| 22| 52| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n| 21| 52| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n| 22| 51| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n| 20| 50| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n| 22| 52| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n| 22| 50| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n| 22| 51| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n| 21| 51| 33|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n| 20| 50| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n| 21| 49| 33|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n| 21| 49| 33|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n| 20| 51| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n| 18| 49| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n| 19| 48| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n| 16| 53| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n| 18| 52| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n| 18| 51| 32|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n+---+---+---+-----------+--------------------+----------+--------------+\nonly showing top 20 rows\n\n"
                }
            ],
            "source": "from pyspark.ml.feature import OneHotEncoder\n\nencoder=OneHotEncoder(inputCol='ClassIndex', outputCol='CategoryVec')\nencoded=encoder.transform(indexed)\n\nencoded.show()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "PS: Indexer is a estimator whereas the one hot encoder is a pure transformer as the indexer has to go through all the strings to label index them"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "4. To convert the values x, y, z (independent variables) into the vectors as the pyspark can only work on vector objects"
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "Exception ignored in: <object repr() failed>\nTraceback (most recent call last):\n  File \"/opt/ibm/spark/python/pyspark/ml/wrapper.py\", line 105, in __del__\n    SparkContext._active_spark_context._gateway.detach(self._java_obj)\nAttributeError: 'VectorAssembler' object has no attribute '_java_obj'\nException ignored in: <object repr() failed>\nTraceback (most recent call last):\n  File \"/opt/ibm/spark/python/pyspark/ml/wrapper.py\", line 105, in __del__\n    SparkContext._active_spark_context._gateway.detach(self._java_obj)\nAttributeError: 'VectorAssembler' object has no attribute '_java_obj'\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+---+---+---+-----------+--------------------+----------+--------------+----------------+\n|  x|  y|  z|      class|              source|ClassIndex|   CategoryVec|        features|\n+---+---+---+-----------+--------------------+----------+--------------+----------------+\n| 22| 49| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,49.0,35.0]|\n| 22| 49| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,49.0,35.0]|\n| 22| 52| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,52.0,35.0]|\n| 22| 52| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,52.0,35.0]|\n| 21| 52| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[21.0,52.0,34.0]|\n| 22| 51| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,51.0,34.0]|\n| 20| 50| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[20.0,50.0,35.0]|\n| 22| 52| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,52.0,34.0]|\n| 22| 50| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,50.0,34.0]|\n| 22| 51| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,51.0,35.0]|\n| 21| 51| 33|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[21.0,51.0,33.0]|\n| 20| 50| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[20.0,50.0,34.0]|\n| 21| 49| 33|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[21.0,49.0,33.0]|\n| 21| 49| 33|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[21.0,49.0,33.0]|\n| 20| 51| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[20.0,51.0,35.0]|\n| 18| 49| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[18.0,49.0,34.0]|\n| 19| 48| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[19.0,48.0,34.0]|\n| 16| 53| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[16.0,53.0,34.0]|\n| 18| 52| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[18.0,52.0,35.0]|\n| 18| 51| 32|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[18.0,51.0,32.0]|\n+---+---+---+-----------+--------------------+----------+--------------+----------------+\nonly showing top 20 rows\n\n"
                }
            ],
            "source": "from pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\n\nvectorAssembler=VectorAssembler(inputCols=['x', 'y', 'z'],\n                               outputCol='features')\nfeatures_vectorized=vectorAssembler.transform(encoded)\n\nfeatures_vectorized.show()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "5. Normalizing the data (optional)"
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+---+---+---+-----------+--------------------+----------+--------------+----------------+--------------------+\n|  x|  y|  z|      class|              source|ClassIndex|   CategoryVec|        features|       features_norm|\n+---+---+---+-----------+--------------------+----------+--------------+----------------+--------------------+\n| 22| 49| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,49.0,35.0]|[0.20754716981132...|\n| 22| 49| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,49.0,35.0]|[0.20754716981132...|\n| 22| 52| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,52.0,35.0]|[0.20183486238532...|\n| 22| 52| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,52.0,35.0]|[0.20183486238532...|\n| 21| 52| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[21.0,52.0,34.0]|[0.19626168224299...|\n| 22| 51| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,51.0,34.0]|[0.20560747663551...|\n| 20| 50| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[20.0,50.0,35.0]|[0.19047619047619...|\n| 22| 52| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,52.0,34.0]|[0.20370370370370...|\n| 22| 50| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,50.0,34.0]|[0.20754716981132...|\n| 22| 51| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,51.0,35.0]|[0.20370370370370...|\n| 21| 51| 33|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[21.0,51.0,33.0]|[0.2,0.4857142857...|\n| 20| 50| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[20.0,50.0,34.0]|[0.19230769230769...|\n| 21| 49| 33|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[21.0,49.0,33.0]|[0.20388349514563...|\n| 21| 49| 33|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[21.0,49.0,33.0]|[0.20388349514563...|\n| 20| 51| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[20.0,51.0,35.0]|[0.18867924528301...|\n| 18| 49| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[18.0,49.0,34.0]|[0.17821782178217...|\n| 19| 48| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[19.0,48.0,34.0]|[0.18811881188118...|\n| 16| 53| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[16.0,53.0,34.0]|[0.15533980582524...|\n| 18| 52| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[18.0,52.0,35.0]|[0.17142857142857...|\n| 18| 51| 32|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[18.0,51.0,32.0]|[0.17821782178217...|\n+---+---+---+-----------+--------------------+----------+--------------+----------------+--------------------+\nonly showing top 20 rows\n\n"
                }
            ],
            "source": "from pyspark.ml.feature import Normalizer\nnormalizer=Normalizer(inputCol=\"features\", outputCol=\"features_norm\", p=1.0)\nnormalized_data=normalizer.transform(features_vectorized)\n\nnormalized_data.show()####LLLLLAAAAZZZZYYYYYYY!!!!!!!"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### CREATING PIPELINE FOR ALL THE  ABOVE IN ONE MOVE:"
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "Exception ignored in: <object repr() failed>\nTraceback (most recent call last):\n  File \"/opt/ibm/spark/python/pyspark/ml/wrapper.py\", line 105, in __del__\n    SparkContext._active_spark_context._gateway.detach(self._java_obj)\nAttributeError: 'Normalizer' object has no attribute '_java_obj'\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+---+---+---+-----------+--------------------+----------+--------------+----------------+--------------------+\n|  x|  y|  z|      class|              source|ClassIndex|   CategoryVec|        features|       features_norm|\n+---+---+---+-----------+--------------------+----------+--------------+----------------+--------------------+\n| 22| 49| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,49.0,35.0]|[0.20754716981132...|\n| 22| 49| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,49.0,35.0]|[0.20754716981132...|\n| 22| 52| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,52.0,35.0]|[0.20183486238532...|\n| 22| 52| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,52.0,35.0]|[0.20183486238532...|\n| 21| 52| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[21.0,52.0,34.0]|[0.19626168224299...|\n| 22| 51| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,51.0,34.0]|[0.20560747663551...|\n| 20| 50| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[20.0,50.0,35.0]|[0.19047619047619...|\n| 22| 52| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,52.0,34.0]|[0.20370370370370...|\n| 22| 50| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,50.0,34.0]|[0.20754716981132...|\n| 22| 51| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,51.0,35.0]|[0.20370370370370...|\n| 21| 51| 33|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[21.0,51.0,33.0]|[0.2,0.4857142857...|\n| 20| 50| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[20.0,50.0,34.0]|[0.19230769230769...|\n| 21| 49| 33|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[21.0,49.0,33.0]|[0.20388349514563...|\n| 21| 49| 33|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[21.0,49.0,33.0]|[0.20388349514563...|\n| 20| 51| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[20.0,51.0,35.0]|[0.18867924528301...|\n| 18| 49| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[18.0,49.0,34.0]|[0.17821782178217...|\n| 19| 48| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[19.0,48.0,34.0]|[0.18811881188118...|\n| 16| 53| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[16.0,53.0,34.0]|[0.15533980582524...|\n| 18| 52| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[18.0,52.0,35.0]|[0.17142857142857...|\n| 18| 51| 32|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[18.0,51.0,32.0]|[0.17821782178217...|\n+---+---+---+-----------+--------------------+----------+--------------+----------------+--------------------+\nonly showing top 20 rows\n\n"
                }
            ],
            "source": "from pyspark.ml import Pipeline\npipeline=Pipeline(stages=[indexer, encoder, vectorAssembler, normalizer])#put in all the stages here.. \nmodel=pipeline.fit(df)\n\nprediction=model.transform(df)\nprediction.show()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Hence, all the above 5 stages' final output matched with this output"
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6 with Spark",
            "language": "python3",
            "name": "python36"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}